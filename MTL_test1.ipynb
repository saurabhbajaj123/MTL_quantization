{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MTL_test1.ipynb","provenance":[],"mount_file_id":"162becatGR4D3yM8eFgIG6N-j1mcVpZrA","authorship_tag":"ABX9TyN17QpiIE1JvODJejnH+tzK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"jb1-r5tiKV3W","executionInfo":{"status":"ok","timestamp":1646006262097,"user_tz":300,"elapsed":12239,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"outputs":[],"source":["import numpy as np\n","import os\n","import copy\n","import itertools\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import random\n","import argparse\n","from pathlib import Path\n","from copy import deepcopy\n","import torchvision.models as models\n","import torchvision\n","\n","\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0qva2SVNBka","executionInfo":{"status":"ok","timestamp":1646006530126,"user_tz":300,"elapsed":144,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}},"outputId":"764f8418-8cac-4b70-c61b-4fcdc32e8d07"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UPDLyspEKaP7","executionInfo":{"status":"ok","timestamp":1646006262380,"user_tz":300,"elapsed":285,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}},"outputId":"8531191c-0539-4380-8ed8-d47ba669a745"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ker14IxvL4iz","executionInfo":{"status":"ok","timestamp":1646006262741,"user_tz":300,"elapsed":362,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}},"outputId":"0cd9dd03-d92f-4544-f816-e310e81ec03f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n"]}]},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/Courses/Sem_2_Spring_22/Independent_study\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8eCRIHNrOHy_","executionInfo":{"status":"ok","timestamp":1646010957641,"user_tz":300,"elapsed":484,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}},"outputId":"eafb2020-24a9-4508-ca93-a395d61f4ca3"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/Courses/Sem_2_Spring_22/Independent_study'\n","/content/drive/MyDrive/Courses/Sem_2_Spring_22/Independent study\n"]}]},{"cell_type":"code","source":["from GitHubRepos.TreeMTL.data.nyuv2_dataloader_adashare import NYU_v2\n","from GitHubRepos.TreeMTL.data.pixel2pixel_loss import NYUCriterions, TaskonomyCriterions\n","from GitHubRepos.TreeMTL.data.pixel2pixel_metrics import NYUMetrics, TaskonomyMetrics\n","from GitHubRepos.TreeMTL.main.trainer import Trainer\n","from GitHubRepos.TreeMTL.main.head import ASPPHeadNode, Classification_Module\n"],"metadata":{"id":"K13_41oWf4BL","executionInfo":{"status":"ok","timestamp":1646006265018,"user_tz":300,"elapsed":2279,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","tasks = ('segment_semantic','normal','depth_zbuffer')\n","task_cls_num = {'segment_semantic': 40, 'normal':3, 'depth_zbuffer': 1}\n","\n","criterionDict = {}\n","metricDict = {}\n","clsNum = {}\n","three_task = ['segment_semantic','normal','depth_zbuffer']\n","\n","dataroot = \"Datasets/nyu_v2\"\n","dataset = NYU_v2(dataroot, 'train', crop_h=321, crop_w=321)\n","trainDataloader = DataLoader(dataset, 16, shuffle=True)\n","\n","dataset = NYU_v2(dataroot, 'test', crop_h=321, crop_w=321)\n","valDataloader = DataLoader(dataset, 16, shuffle=True)\n","\n","for task in three_task:\n","    criterionDict[task] = NYUCriterions(task)\n","    metricDict[task] = NYUMetrics(task)\n","    clsNum[task] = task_cls_num[task]\n","print(criterionDict, metricDict, clsNum)"],"metadata":{"id":"Sd4arDB7fHMk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646006265018,"user_tz":300,"elapsed":5,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}},"outputId":"b385f6d2-2b0f-4768-fb26-d9c65b28eb3b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{'segment_semantic': NYUCriterions(\n","  (cosine_similiarity): CosineSimilarity()\n","  (l1_loss): L1Loss()\n","  (l1_loss_sum): L1Loss()\n","  (cross_entropy): CrossEntropyLoss()\n","), 'normal': NYUCriterions(\n","  (cosine_similiarity): CosineSimilarity()\n","  (l1_loss): L1Loss()\n","  (l1_loss_sum): L1Loss()\n","  (cross_entropy): CrossEntropyLoss()\n","), 'depth_zbuffer': NYUCriterions(\n","  (cosine_similiarity): CosineSimilarity()\n","  (l1_loss): L1Loss()\n","  (l1_loss_sum): L1Loss()\n","  (cross_entropy): CrossEntropyLoss()\n",")} {'segment_semantic': <GitHubRepos.TreeMTL.data.pixel2pixel_metrics.NYUMetrics object at 0x7fdb11629fd0>, 'normal': <GitHubRepos.TreeMTL.data.pixel2pixel_metrics.NYUMetrics object at 0x7fdaf9d29a50>, 'depth_zbuffer': <GitHubRepos.TreeMTL.data.pixel2pixel_metrics.NYUMetrics object at 0x7fdaf9d29d50>} {'segment_semantic': 40, 'normal': 3, 'depth_zbuffer': 1}\n"]}]},{"cell_type":"code","source":["# data = next(iter(trainDataloader))\n","# x = data['input']\n","# y_normal = data['normal']\n","# print(x.shape)\n","# print(y_normal.shape)"],"metadata":{"id":"xCctVnobPXLg","executionInfo":{"status":"ok","timestamp":1646006265195,"user_tz":300,"elapsed":180,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Importing a pre-trained resnet"],"metadata":{"id":"mHKjglWyYBJv"}},{"cell_type":"code","source":["class MTLModel(nn.Module):\n","  def __init__(self, feature_dim=512, clsNum={}, backbone_init=None):\n","    super(MTLModel, self).__init__()\n","    self.backbone = backbone_init\n","    self.heads = nn.ModuleDict()\n","    # clsNum will contian the name of the task and the number of classes in that task\n","    for task in clsNum:\n","      self.heads[task] = ASPPHeadNode(feature_dim, clsNum[task])\n","  \n","  def forward(self, x):\n","    features = self.backbone(x)\n","    output = {}\n","    idx = 0\n","    for task in self.heads:\n","      output[task] = self.heads[task](features) # why use idx?\n","      # idx += 1\n","    return output"],"metadata":{"id":"awgPB1X4kM4r","executionInfo":{"status":"ok","timestamp":1646006270954,"user_tz":300,"elapsed":163,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["model_ft = models.resnet34(pretrained=True)\n","### strip the last layer\n","feature_extractor = torch.nn.Sequential(*list(model_ft.children())[:-1])\n","### check this works\n","# x = torch.randn([1,3,224,224])\n","# features = feature_extractor(x) # output now has the features corresponding to input x\n","# print(features.shape)"],"metadata":{"id":"tp2zqeoCZnEd","executionInfo":{"status":"ok","timestamp":1646006274107,"user_tz":300,"elapsed":1232,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model = MTLModel(512, clsNum, feature_extractor)\n","# print(model)"],"metadata":{"id":"h9BGmYK5mmiA","executionInfo":{"status":"ok","timestamp":1646006280374,"user_tz":300,"elapsed":920,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, feature_extractor.parameters()), lr=0.001, betas=(0.5, 0.999), weight_decay=0.0001)\n","optimizer.zero_grad()"],"metadata":{"id":"We5IZW25ZXRU","executionInfo":{"status":"ok","timestamp":1646006286149,"user_tz":300,"elapsed":138,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["loss_lambda = {'segment_semantic': 1, 'normal':1, 'depth_zbuffer': 1}\n","checkpoint = 'Checkpoints/NYUv2/test/'"],"metadata":{"id":"8gSd-kpbNCeM","executionInfo":{"status":"ok","timestamp":1646006287524,"user_tz":300,"elapsed":183,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# loss = 0\n","# y = {}\n","# output = model(x)\n","# for task in tasks:\n","# # task = \"normal\"\n","# # task = \"segment_semantic\"\n","# # task = \"depth_zbuffer\"\n","#   tloss = criterionDict[task](output[task], data[task])\n","#   print(\"{} loss= {}\".format(task, tloss.item())) \n","#   loss += loss_lambda[task]*tloss\n","# loss.backward()\n","# optimizer.step()"],"metadata":{"id":"EjYeX4DQZr6v","executionInfo":{"status":"ok","timestamp":1646006288777,"user_tz":300,"elapsed":145,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# tb = SummaryWriter()\n","# data = next(iter(trainDataloader))\n","# \n"],"metadata":{"id":"MA0az_seGdQm","executionInfo":{"status":"ok","timestamp":1646006299971,"user_tz":300,"elapsed":196,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# grid = torchvision.utils.make_grid(data['input'])\n","# print(grid)"],"metadata":{"id":"sSuN5sT5Hyrq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tb = {} \n","tb = SummaryWriter()\n","# tb.add_image('images', grid)\n","# tb.add_graph(feature_extractor, data['input'])\n","# tb.close()"],"metadata":{"id":"MmLqetmTH7xD","executionInfo":{"status":"ok","timestamp":1646006309955,"user_tz":300,"elapsed":3327,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sys import exit\n","import torch\n","\n","class Trainer():\n","    def __init__(self, model, tasks, train_dataloader, val_dataloader, criterion_dict, metric_dict, \n","                 lr=0.001, decay_lr_freq=4000, decay_lr_rate=0.5,\n","                 print_iters=50, val_iters=200, save_iters=200):\n","        print(\"initializing the Trainer\")\n","        super(Trainer, self).__init__()\n","        self.model = model\n","        self.startIter = 0\n","        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr, betas=(0.5, 0.999), weight_decay=0.0001)\n","        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=decay_lr_freq, gamma=decay_lr_rate)\n","        \n","        self.tasks = tasks\n","        \n","        self.train_dataloader = train_dataloader\n","        self.train_iter = iter(self.train_dataloader)\n","        self.val_dataloader = val_dataloader\n","        self.criterion_dict = criterion_dict\n","        self.metric_dict = metric_dict\n","        \n","        self.loss_list = {}\n","        self.set_train_loss()\n","        \n","        self.print_iters = print_iters\n","        self.val_iters = val_iters\n","        self.save_iters = save_iters\n","        self.tb_data = next(iter(trainDataloader))\n","\n","    def train(self, iters, loss_lambda, savePath=None, reload=None):\n","        self.model.train()\n","        if reload is not None and reload != 'false' and savePath is not None:\n","            self.load_model(savePath, reload)\n","        # tb = SummaryWriter()\n","        # if self.tb_data['input']:\n","        grid = torchvision.utils.make_grid(self.tb_data['input'])\n","        tb.add_image('images', grid)\n","        tb.add_graph(feature_extractor, self.tb_data['input'])\n","        \n","\n","        \n","        for i in range(self.startIter, iters):\n","            print('i = {}'.format(i))\n","            self.train_step(loss_lambda)\n","\n","            if (i+1) % self.print_iters == 0:\n","                self.print_train_loss(i)\n","                tb.add_scalar('Loss', self.get_loss(i), i)\n","                self.set_train_loss()\n","            if (i+1) % self.val_iters == 0:\n","                self.validate(i)\n","            if (i+1) % self.save_iters == 0:\n","                if savePath is not None:\n","                    self.save_model(i, savePath)\n","            # print(self.get_loss(i))\n","            # tb.add_scalar('Loss', self.get_loss(i), i)\n","            tb.add_histogram('conv1.weight', feature_extractor[0].weight, i)\n","            # tb.close()\n","        # Reset loss list and the data iters\n","        self.set_train_loss()\n","        return\n","    \n","    def train_step(self, loss_lambda):\n","        print(\"starting train step\")\n","        self.model.train()\n","\n","\n","        try:\n","            data = next(self.train_iter)\n","        except StopIteration:\n","            self.train_iter = iter(self.train_dataloader)\n","            data = next(self.train_iter)\n","        \n","        x = data['input']#.cuda()\n","        # print(\"size(x) = \", x.size())\n","        self.optimizer.zero_grad()\n","        output = self.model(x)\n","        # print(\"output = {}\".format(output.size()))\n","        loss = 0\n","        for task in self.tasks:\n","            # print(\"task = {}\".format(task))\n","            y = data[task]#.cuda()\n","            if task + '_mask' in data:\n","                tloss = self.criterion_dict[task](output[task], y, data[task + '_mask'])#.cuda())\n","            else:\n","                tloss = self.criterion_dict[task](output[task], y)\n","                \n","            self.loss_list[task].append(tloss.item())\n","            loss += loss_lambda[task] * tloss\n","            # print(\"loss = {}\".format(loss))\n","        self.loss_list['total'].append(loss.item())\n","        print(self.loss_list['total'])\n","        \n","        loss.backward()\n","        self.optimizer.step()\n","        if self.scheduler is not None:\n","            self.scheduler.step()\n","        return\n","    \n","    def validate(self, it):\n","        self.model.eval()\n","        loss_list = {}\n","        for task in self.tasks:\n","            loss_list[task] = []\n","        \n","        for i, data in enumerate(self.val_dataloader):\n","            x = data['input']#.cuda()\n","            output = self.model(x)\n","\n","            for task in self.tasks:\n","                y = data[task]#.cuda()\n","                if task + '_mask' in data:\n","                    tloss = self.criterion_dict[task](output[task], y, data[task + '_mask'])#.cuda())\n","                    self.metric_dict[task](output[task], y, data[task + '_mask'])#.cuda())\n","                else:\n","                    tloss = self.criterion_dict[task](output[task], y)\n","                    self.metric_dict[task](output[task], y)\n","                loss_list[task].append(tloss.item())\n","\n","        for task in self.tasks:\n","            val_results = self.metric_dict[task].val_metrics()\n","            print('[Iter {} Task {}] Val Loss: {:.4f}'.format((it+1), task[:4], np.mean(loss_list[task])), flush=True)\n","            print(val_results, flush=True)\n","        print('======================================================================', flush=True)\n","        return\n","    \n","    # helper functions\n","    def set_train_loss(self):\n","        for task in self.tasks:\n","            self.loss_list[task] = []\n","        self.loss_list['total'] = []\n","        return\n","    \n","    def load_model(self, savePath, reload):\n","        model_name = True\n","        for task in self.tasks:\n","            if task not in reload:\n","                model_name = False\n","                break\n","        if model_name:\n","            state = torch.load(savePath + reload)\n","            self.startIter = state['iter'] + 1\n","            self.model.load_state_dict(state['state_dict'])\n","            self.optimizer.load_state_dict(state['optimizer'])\n","            self.scheduler.load_state_dict(state['scheduler'])\n","        else:\n","            print('Cannot load from models trained from different tasks.')\n","            exit()\n","        return\n","    \n","    def save_model(self, it, savePath):\n","        state = {'iter': it,\n","                'state_dict': self.model.state_dict(),\n","                'layout': self.model.layout,\n","                'optimizer': self.optimizer.state_dict(),\n","                'scheduler': self.scheduler.state_dict()}\n","        if hasattr(self.model, 'branch') and self.model.branch is not None:\n","            torch.save(state, savePath + '_'.join(self.tasks) + '_b' + str(self.model.branch) + '.model')\n","        elif hasattr(self.model, 'layout') and self.model.layout is not None:\n","            torch.save(state, savePath + '_'.join(self.tasks) + '.model')\n","        return\n","    \n","    def print_train_loss(self, it):\n","        # Function: Print loss for each task\n","        for task in self.tasks:\n","            if self.loss_list[task]:\n","                avg_loss = np.mean(self.loss_list[task])\n","            else:\n","                continue\n","            print('[Iter {} Task {}] Train Loss: {:.4f}'.format((it+1), task[:4], avg_loss), flush=True)\n","        print('[Iter {} Total] Train Loss: {:.4f}'.format((it+1), np.mean(self.loss_list['total'])), flush=True)\n","        print('======================================================================', flush=True)\n","        return\n","    def get_loss(self, it):\n","        # for task in self.tasks:\n","        #     if self.loss_list[task]:\n","        #         avg_loss = np.mean(self.loss_list[task])\n","        #     else:\n","        #         continue\n","        #     print('[Iter {} Task {}] Train Loss: {:.4f}'.format((it+1), task[:4], avg_loss), flush=True)\n","        print('[Iter {} Total] Train Loss: {:.4f}'.format((it+1), np.mean(self.loss_list['total'])), flush=True)\n","        # print('======================================================================', flush=True)\n","        return np.mean(self.loss_list['total'])"],"metadata":{"id":"T7PnEjfer9IP","executionInfo":{"status":"ok","timestamp":1646006312409,"user_tz":300,"elapsed":536,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(model, three_task, trainDataloader, valDataloader, criterionDict, metricDict, print_iters=1)\n","trainer.train(100, loss_lambda, checkpoint)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6j1Qp6xtTZqG","outputId":"71fea272-f4b8-4bf3-812c-ae391594baa7","executionInfo":{"status":"ok","timestamp":1646008905203,"user_tz":300,"elapsed":2353717,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["initializing the Trainer\n","i = 0\n","starting train step\n","[5.180189609527588]\n","[Iter 1 Task segm] Train Loss: 2.5278\n","[Iter 1 Task norm] Train Loss: 0.2082\n","[Iter 1 Task dept] Train Loss: 2.4442\n","[Iter 1 Total] Train Loss: 5.1802\n","======================================================================\n","[Iter 1 Total] Train Loss: 5.1802\n","i = 1\n","starting train step\n","[7.208876609802246]\n","[Iter 2 Task segm] Train Loss: 2.8601\n","[Iter 2 Task norm] Train Loss: 0.0987\n","[Iter 2 Task dept] Train Loss: 4.2501\n","[Iter 2 Total] Train Loss: 7.2089\n","======================================================================\n","[Iter 2 Total] Train Loss: 7.2089\n","i = 2\n","starting train step\n","[5.326625823974609]\n","[Iter 3 Task segm] Train Loss: 2.8814\n","[Iter 3 Task norm] Train Loss: 0.1618\n","[Iter 3 Task dept] Train Loss: 2.2834\n","[Iter 3 Total] Train Loss: 5.3266\n","======================================================================\n","[Iter 3 Total] Train Loss: 5.3266\n","i = 3\n","starting train step\n","[4.552000522613525]\n","[Iter 4 Task segm] Train Loss: 2.8759\n","[Iter 4 Task norm] Train Loss: 0.1272\n","[Iter 4 Task dept] Train Loss: 1.5490\n","[Iter 4 Total] Train Loss: 4.5520\n","======================================================================\n","[Iter 4 Total] Train Loss: 4.5520\n","i = 4\n","starting train step\n","[3.0558533668518066]\n","[Iter 5 Task segm] Train Loss: 1.8369\n","[Iter 5 Task norm] Train Loss: 0.0310\n","[Iter 5 Task dept] Train Loss: 1.1880\n","[Iter 5 Total] Train Loss: 3.0559\n","======================================================================\n","[Iter 5 Total] Train Loss: 3.0559\n","i = 5\n","starting train step\n","[3.527320384979248]\n","[Iter 6 Task segm] Train Loss: 2.4849\n","[Iter 6 Task norm] Train Loss: 0.0477\n","[Iter 6 Task dept] Train Loss: 0.9947\n","[Iter 6 Total] Train Loss: 3.5273\n","======================================================================\n","[Iter 6 Total] Train Loss: 3.5273\n","i = 6\n","starting train step\n","[3.5835137367248535]\n","[Iter 7 Task segm] Train Loss: 2.7007\n","[Iter 7 Task norm] Train Loss: 0.0570\n","[Iter 7 Task dept] Train Loss: 0.8259\n","[Iter 7 Total] Train Loss: 3.5835\n","======================================================================\n","[Iter 7 Total] Train Loss: 3.5835\n","i = 7\n","starting train step\n","[5.014965534210205]\n","[Iter 8 Task segm] Train Loss: 2.9683\n","[Iter 8 Task norm] Train Loss: 0.1020\n","[Iter 8 Task dept] Train Loss: 1.9447\n","[Iter 8 Total] Train Loss: 5.0150\n","======================================================================\n","[Iter 8 Total] Train Loss: 5.0150\n","i = 8\n","starting train step\n","[4.392240524291992]\n","[Iter 9 Task segm] Train Loss: 2.6318\n","[Iter 9 Task norm] Train Loss: 0.1204\n","[Iter 9 Task dept] Train Loss: 1.6401\n","[Iter 9 Total] Train Loss: 4.3922\n","======================================================================\n","[Iter 9 Total] Train Loss: 4.3922\n","i = 9\n","starting train step\n","[3.968787908554077]\n","[Iter 10 Task segm] Train Loss: 2.3669\n","[Iter 10 Task norm] Train Loss: 0.1864\n","[Iter 10 Task dept] Train Loss: 1.4156\n","[Iter 10 Total] Train Loss: 3.9688\n","======================================================================\n","[Iter 10 Total] Train Loss: 3.9688\n","i = 10\n","starting train step\n","[4.701145172119141]\n","[Iter 11 Task segm] Train Loss: 3.4368\n","[Iter 11 Task norm] Train Loss: 0.1334\n","[Iter 11 Task dept] Train Loss: 1.1310\n","[Iter 11 Total] Train Loss: 4.7011\n","======================================================================\n","[Iter 11 Total] Train Loss: 4.7011\n","i = 11\n","starting train step\n","[3.2425894737243652]\n","[Iter 12 Task segm] Train Loss: 2.2071\n","[Iter 12 Task norm] Train Loss: 0.1099\n","[Iter 12 Task dept] Train Loss: 0.9256\n","[Iter 12 Total] Train Loss: 3.2426\n","======================================================================\n","[Iter 12 Total] Train Loss: 3.2426\n","i = 12\n","starting train step\n","[4.877897262573242]\n","[Iter 13 Task segm] Train Loss: 3.0931\n","[Iter 13 Task norm] Train Loss: 0.1352\n","[Iter 13 Task dept] Train Loss: 1.6497\n","[Iter 13 Total] Train Loss: 4.8779\n","======================================================================\n","[Iter 13 Total] Train Loss: 4.8779\n","i = 13\n","starting train step\n","[3.9971837997436523]\n","[Iter 14 Task segm] Train Loss: 2.6678\n","[Iter 14 Task norm] Train Loss: 0.0558\n","[Iter 14 Task dept] Train Loss: 1.2736\n","[Iter 14 Total] Train Loss: 3.9972\n","======================================================================\n","[Iter 14 Total] Train Loss: 3.9972\n","i = 14\n","starting train step\n","[3.417820930480957]\n","[Iter 15 Task segm] Train Loss: 1.8612\n","[Iter 15 Task norm] Train Loss: 0.1167\n","[Iter 15 Task dept] Train Loss: 1.4399\n","[Iter 15 Total] Train Loss: 3.4178\n","======================================================================\n","[Iter 15 Total] Train Loss: 3.4178\n","i = 15\n","starting train step\n","[4.362637519836426]\n","[Iter 16 Task segm] Train Loss: 2.1198\n","[Iter 16 Task norm] Train Loss: 0.0974\n","[Iter 16 Task dept] Train Loss: 2.1454\n","[Iter 16 Total] Train Loss: 4.3626\n","======================================================================\n","[Iter 16 Total] Train Loss: 4.3626\n","i = 16\n","starting train step\n","[5.722433090209961]\n","[Iter 17 Task segm] Train Loss: 3.8224\n","[Iter 17 Task norm] Train Loss: 0.0885\n","[Iter 17 Task dept] Train Loss: 1.8115\n","[Iter 17 Total] Train Loss: 5.7224\n","======================================================================\n","[Iter 17 Total] Train Loss: 5.7224\n","i = 17\n","starting train step\n","[3.323180675506592]\n","[Iter 18 Task segm] Train Loss: 2.0270\n","[Iter 18 Task norm] Train Loss: 0.0395\n","[Iter 18 Task dept] Train Loss: 1.2567\n","[Iter 18 Total] Train Loss: 3.3232\n","======================================================================\n","[Iter 18 Total] Train Loss: 3.3232\n","i = 18\n","starting train step\n","[3.8169755935668945]\n","[Iter 19 Task segm] Train Loss: 2.2895\n","[Iter 19 Task norm] Train Loss: 0.0357\n","[Iter 19 Task dept] Train Loss: 1.4919\n","[Iter 19 Total] Train Loss: 3.8170\n","======================================================================\n","[Iter 19 Total] Train Loss: 3.8170\n","i = 19\n","starting train step\n","[3.434584617614746]\n","[Iter 20 Task segm] Train Loss: 2.1792\n","[Iter 20 Task norm] Train Loss: 0.0856\n","[Iter 20 Task dept] Train Loss: 1.1699\n","[Iter 20 Total] Train Loss: 3.4346\n","======================================================================\n","[Iter 20 Total] Train Loss: 3.4346\n","i = 20\n","starting train step\n","[3.7036266326904297]\n","[Iter 21 Task segm] Train Loss: 2.5520\n","[Iter 21 Task norm] Train Loss: 0.1240\n","[Iter 21 Task dept] Train Loss: 1.0276\n","[Iter 21 Total] Train Loss: 3.7036\n","======================================================================\n","[Iter 21 Total] Train Loss: 3.7036\n","i = 21\n","starting train step\n","[3.3958425521850586]\n","[Iter 22 Task segm] Train Loss: 1.7761\n","[Iter 22 Task norm] Train Loss: 0.0564\n","[Iter 22 Task dept] Train Loss: 1.5633\n","[Iter 22 Total] Train Loss: 3.3958\n","======================================================================\n","[Iter 22 Total] Train Loss: 3.3958\n","i = 22\n","starting train step\n","[3.8345088958740234]\n","[Iter 23 Task segm] Train Loss: 2.8766\n","[Iter 23 Task norm] Train Loss: 0.0793\n","[Iter 23 Task dept] Train Loss: 0.8787\n","[Iter 23 Total] Train Loss: 3.8345\n","======================================================================\n","[Iter 23 Total] Train Loss: 3.8345\n","i = 23\n","starting train step\n","[4.246343612670898]\n","[Iter 24 Task segm] Train Loss: 3.1820\n","[Iter 24 Task norm] Train Loss: 0.0364\n","[Iter 24 Task dept] Train Loss: 1.0279\n","[Iter 24 Total] Train Loss: 4.2463\n","======================================================================\n","[Iter 24 Total] Train Loss: 4.2463\n","i = 24\n","starting train step\n","[3.674089193344116]\n","[Iter 25 Task segm] Train Loss: 2.4634\n","[Iter 25 Task norm] Train Loss: 0.2314\n","[Iter 25 Task dept] Train Loss: 0.9792\n","[Iter 25 Total] Train Loss: 3.6741\n","======================================================================\n","[Iter 25 Total] Train Loss: 3.6741\n","i = 25\n","starting train step\n","[3.91790509223938]\n","[Iter 26 Task segm] Train Loss: 2.5526\n","[Iter 26 Task norm] Train Loss: 0.0638\n","[Iter 26 Task dept] Train Loss: 1.3015\n","[Iter 26 Total] Train Loss: 3.9179\n","======================================================================\n","[Iter 26 Total] Train Loss: 3.9179\n","i = 26\n","starting train step\n","[4.252054691314697]\n","[Iter 27 Task segm] Train Loss: 2.4563\n","[Iter 27 Task norm] Train Loss: 0.0296\n","[Iter 27 Task dept] Train Loss: 1.7661\n","[Iter 27 Total] Train Loss: 4.2521\n","======================================================================\n","[Iter 27 Total] Train Loss: 4.2521\n","i = 27\n","starting train step\n","[4.6255011558532715]\n","[Iter 28 Task segm] Train Loss: 3.3453\n","[Iter 28 Task norm] Train Loss: 0.0652\n","[Iter 28 Task dept] Train Loss: 1.2150\n","[Iter 28 Total] Train Loss: 4.6255\n","======================================================================\n","[Iter 28 Total] Train Loss: 4.6255\n","i = 28\n","starting train step\n","[3.401293992996216]\n","[Iter 29 Task segm] Train Loss: 2.4426\n","[Iter 29 Task norm] Train Loss: 0.0636\n","[Iter 29 Task dept] Train Loss: 0.8951\n","[Iter 29 Total] Train Loss: 3.4013\n","======================================================================\n","[Iter 29 Total] Train Loss: 3.4013\n","i = 29\n","starting train step\n","[4.876489639282227]\n","[Iter 30 Task segm] Train Loss: 2.5876\n","[Iter 30 Task norm] Train Loss: 0.1928\n","[Iter 30 Task dept] Train Loss: 2.0960\n","[Iter 30 Total] Train Loss: 4.8765\n","======================================================================\n","[Iter 30 Total] Train Loss: 4.8765\n","i = 30\n","starting train step\n","[3.0319175720214844]\n","[Iter 31 Task segm] Train Loss: 1.5645\n","[Iter 31 Task norm] Train Loss: 0.1328\n","[Iter 31 Task dept] Train Loss: 1.3346\n","[Iter 31 Total] Train Loss: 3.0319\n","======================================================================\n","[Iter 31 Total] Train Loss: 3.0319\n","i = 31\n","starting train step\n","[4.526485443115234]\n","[Iter 32 Task segm] Train Loss: 2.9229\n","[Iter 32 Task norm] Train Loss: 0.1162\n","[Iter 32 Task dept] Train Loss: 1.4873\n","[Iter 32 Total] Train Loss: 4.5265\n","======================================================================\n","[Iter 32 Total] Train Loss: 4.5265\n","i = 32\n","starting train step\n","[3.454897403717041]\n","[Iter 33 Task segm] Train Loss: 2.3541\n","[Iter 33 Task norm] Train Loss: 0.0582\n","[Iter 33 Task dept] Train Loss: 1.0426\n","[Iter 33 Total] Train Loss: 3.4549\n","======================================================================\n","[Iter 33 Total] Train Loss: 3.4549\n","i = 33\n","starting train step\n","[3.7278943061828613]\n","[Iter 34 Task segm] Train Loss: 2.4514\n","[Iter 34 Task norm] Train Loss: 0.1225\n","[Iter 34 Task dept] Train Loss: 1.1539\n","[Iter 34 Total] Train Loss: 3.7279\n","======================================================================\n","[Iter 34 Total] Train Loss: 3.7279\n","i = 34\n","starting train step\n","[4.820467948913574]\n","[Iter 35 Task segm] Train Loss: 3.4455\n","[Iter 35 Task norm] Train Loss: 0.1285\n","[Iter 35 Task dept] Train Loss: 1.2465\n","[Iter 35 Total] Train Loss: 4.8205\n","======================================================================\n","[Iter 35 Total] Train Loss: 4.8205\n","i = 35\n","starting train step\n","[2.9734411239624023]\n","[Iter 36 Task segm] Train Loss: 2.0239\n","[Iter 36 Task norm] Train Loss: 0.0691\n","[Iter 36 Task dept] Train Loss: 0.8804\n","[Iter 36 Total] Train Loss: 2.9734\n","======================================================================\n","[Iter 36 Total] Train Loss: 2.9734\n","i = 36\n","starting train step\n","[3.960111618041992]\n","[Iter 37 Task segm] Train Loss: 1.4105\n","[Iter 37 Task norm] Train Loss: 0.0839\n","[Iter 37 Task dept] Train Loss: 2.4657\n","[Iter 37 Total] Train Loss: 3.9601\n","======================================================================\n","[Iter 37 Total] Train Loss: 3.9601\n","i = 37\n","starting train step\n","[4.23122501373291]\n","[Iter 38 Task segm] Train Loss: 2.8651\n","[Iter 38 Task norm] Train Loss: 0.0327\n","[Iter 38 Task dept] Train Loss: 1.3335\n","[Iter 38 Total] Train Loss: 4.2312\n","======================================================================\n","[Iter 38 Total] Train Loss: 4.2312\n","i = 38\n","starting train step\n","[3.1472959518432617]\n","[Iter 39 Task segm] Train Loss: 1.5938\n","[Iter 39 Task norm] Train Loss: 0.1719\n","[Iter 39 Task dept] Train Loss: 1.3816\n","[Iter 39 Total] Train Loss: 3.1473\n","======================================================================\n","[Iter 39 Total] Train Loss: 3.1473\n","i = 39\n","starting train step\n","[4.8688812255859375]\n","[Iter 40 Task segm] Train Loss: 3.4554\n","[Iter 40 Task norm] Train Loss: 0.0966\n","[Iter 40 Task dept] Train Loss: 1.3169\n","[Iter 40 Total] Train Loss: 4.8689\n","======================================================================\n","[Iter 40 Total] Train Loss: 4.8689\n","i = 40\n","starting train step\n","[4.6164469718933105]\n","[Iter 41 Task segm] Train Loss: 2.8511\n","[Iter 41 Task norm] Train Loss: 0.1386\n","[Iter 41 Task dept] Train Loss: 1.6267\n","[Iter 41 Total] Train Loss: 4.6164\n","======================================================================\n","[Iter 41 Total] Train Loss: 4.6164\n","i = 41\n","starting train step\n","[3.411156177520752]\n","[Iter 42 Task segm] Train Loss: 2.5967\n","[Iter 42 Task norm] Train Loss: 0.0797\n","[Iter 42 Task dept] Train Loss: 0.7348\n","[Iter 42 Total] Train Loss: 3.4112\n","======================================================================\n","[Iter 42 Total] Train Loss: 3.4112\n","i = 42\n","starting train step\n","[3.4201676845550537]\n","[Iter 43 Task segm] Train Loss: 2.0292\n","[Iter 43 Task norm] Train Loss: 0.1078\n","[Iter 43 Task dept] Train Loss: 1.2831\n","[Iter 43 Total] Train Loss: 3.4202\n","======================================================================\n","[Iter 43 Total] Train Loss: 3.4202\n","i = 43\n","starting train step\n","[4.683244705200195]\n","[Iter 44 Task segm] Train Loss: 2.4094\n","[Iter 44 Task norm] Train Loss: 0.0704\n","[Iter 44 Task dept] Train Loss: 2.2034\n","[Iter 44 Total] Train Loss: 4.6832\n","======================================================================\n","[Iter 44 Total] Train Loss: 4.6832\n","i = 44\n","starting train step\n","[4.8794660568237305]\n","[Iter 45 Task segm] Train Loss: 3.2409\n","[Iter 45 Task norm] Train Loss: 0.0527\n","[Iter 45 Task dept] Train Loss: 1.5858\n","[Iter 45 Total] Train Loss: 4.8795\n","======================================================================\n","[Iter 45 Total] Train Loss: 4.8795\n","i = 45\n","starting train step\n","[4.366860866546631]\n","[Iter 46 Task segm] Train Loss: 2.9532\n","[Iter 46 Task norm] Train Loss: 0.0951\n","[Iter 46 Task dept] Train Loss: 1.3185\n","[Iter 46 Total] Train Loss: 4.3669\n","======================================================================\n","[Iter 46 Total] Train Loss: 4.3669\n","i = 46\n","starting train step\n","[4.338752746582031]\n","[Iter 47 Task segm] Train Loss: 2.9913\n","[Iter 47 Task norm] Train Loss: 0.0639\n","[Iter 47 Task dept] Train Loss: 1.2835\n","[Iter 47 Total] Train Loss: 4.3388\n","======================================================================\n","[Iter 47 Total] Train Loss: 4.3388\n","i = 47\n","starting train step\n","[3.871619701385498]\n","[Iter 48 Task segm] Train Loss: 2.6005\n","[Iter 48 Task norm] Train Loss: 0.0551\n","[Iter 48 Task dept] Train Loss: 1.2160\n","[Iter 48 Total] Train Loss: 3.8716\n","======================================================================\n","[Iter 48 Total] Train Loss: 3.8716\n","i = 48\n","starting train step\n","[3.377199649810791]\n","[Iter 49 Task segm] Train Loss: 2.3996\n","[Iter 49 Task norm] Train Loss: 0.0978\n","[Iter 49 Task dept] Train Loss: 0.8798\n","[Iter 49 Total] Train Loss: 3.3772\n","======================================================================\n","[Iter 49 Total] Train Loss: 3.3772\n","i = 49\n","starting train step\n","[3.9701571464538574]\n","[Iter 50 Task segm] Train Loss: 2.9035\n","[Iter 50 Task norm] Train Loss: 0.1396\n","[Iter 50 Task dept] Train Loss: 0.9270\n","[Iter 50 Total] Train Loss: 3.9702\n","======================================================================\n","[Iter 50 Total] Train Loss: 3.9702\n","i = 50\n","starting train step\n","[4.745176315307617]\n","[Iter 51 Task segm] Train Loss: 2.6911\n","[Iter 51 Task norm] Train Loss: 0.0348\n","[Iter 51 Task dept] Train Loss: 2.0193\n","[Iter 51 Total] Train Loss: 4.7452\n","======================================================================\n","[Iter 51 Total] Train Loss: 4.7452\n","i = 51\n","starting train step\n","[4.043733596801758]\n","[Iter 52 Task segm] Train Loss: 2.7698\n","[Iter 52 Task norm] Train Loss: 0.1793\n","[Iter 52 Task dept] Train Loss: 1.0947\n","[Iter 52 Total] Train Loss: 4.0437\n","======================================================================\n","[Iter 52 Total] Train Loss: 4.0437\n","i = 52\n","starting train step\n","[2.9778733253479004]\n","[Iter 53 Task segm] Train Loss: 1.3526\n","[Iter 53 Task norm] Train Loss: 0.1168\n","[Iter 53 Task dept] Train Loss: 1.5084\n","[Iter 53 Total] Train Loss: 2.9779\n","======================================================================\n","[Iter 53 Total] Train Loss: 2.9779\n","i = 53\n","starting train step\n","[5.456233978271484]\n","[Iter 54 Task segm] Train Loss: 3.4977\n","[Iter 54 Task norm] Train Loss: 0.1261\n","[Iter 54 Task dept] Train Loss: 1.8324\n","[Iter 54 Total] Train Loss: 5.4562\n","======================================================================\n","[Iter 54 Total] Train Loss: 5.4562\n","i = 54\n","starting train step\n","[3.765475273132324]\n","[Iter 55 Task segm] Train Loss: 2.1133\n","[Iter 55 Task norm] Train Loss: 0.1148\n","[Iter 55 Task dept] Train Loss: 1.5374\n","[Iter 55 Total] Train Loss: 3.7655\n","======================================================================\n","[Iter 55 Total] Train Loss: 3.7655\n","i = 55\n","starting train step\n","[4.445189952850342]\n","[Iter 56 Task segm] Train Loss: 2.6260\n","[Iter 56 Task norm] Train Loss: 0.0647\n","[Iter 56 Task dept] Train Loss: 1.7545\n","[Iter 56 Total] Train Loss: 4.4452\n","======================================================================\n","[Iter 56 Total] Train Loss: 4.4452\n","i = 56\n","starting train step\n","[4.594770431518555]\n","[Iter 57 Task segm] Train Loss: 3.3749\n","[Iter 57 Task norm] Train Loss: 0.0589\n","[Iter 57 Task dept] Train Loss: 1.1609\n","[Iter 57 Total] Train Loss: 4.5948\n","======================================================================\n","[Iter 57 Total] Train Loss: 4.5948\n","i = 57\n","starting train step\n","[3.5613675117492676]\n","[Iter 58 Task segm] Train Loss: 2.4301\n","[Iter 58 Task norm] Train Loss: 0.0845\n","[Iter 58 Task dept] Train Loss: 1.0467\n","[Iter 58 Total] Train Loss: 3.5614\n","======================================================================\n","[Iter 58 Total] Train Loss: 3.5614\n","i = 58\n","starting train step\n","[3.9503073692321777]\n","[Iter 59 Task segm] Train Loss: 2.2979\n","[Iter 59 Task norm] Train Loss: 0.0620\n","[Iter 59 Task dept] Train Loss: 1.5903\n","[Iter 59 Total] Train Loss: 3.9503\n","======================================================================\n","[Iter 59 Total] Train Loss: 3.9503\n","i = 59\n","starting train step\n","[2.915177583694458]\n","[Iter 60 Task segm] Train Loss: 1.8766\n","[Iter 60 Task norm] Train Loss: 0.0736\n","[Iter 60 Task dept] Train Loss: 0.9650\n","[Iter 60 Total] Train Loss: 2.9152\n","======================================================================\n","[Iter 60 Total] Train Loss: 2.9152\n","i = 60\n","starting train step\n","[4.331177711486816]\n","[Iter 61 Task segm] Train Loss: 2.9192\n","[Iter 61 Task norm] Train Loss: 0.0707\n","[Iter 61 Task dept] Train Loss: 1.3412\n","[Iter 61 Total] Train Loss: 4.3312\n","======================================================================\n","[Iter 61 Total] Train Loss: 4.3312\n","i = 61\n","starting train step\n","[3.8034958839416504]\n","[Iter 62 Task segm] Train Loss: 2.6506\n","[Iter 62 Task norm] Train Loss: 0.0301\n","[Iter 62 Task dept] Train Loss: 1.1227\n","[Iter 62 Total] Train Loss: 3.8035\n","======================================================================\n","[Iter 62 Total] Train Loss: 3.8035\n","i = 62\n","starting train step\n","[3.409075975418091]\n","[Iter 63 Task segm] Train Loss: 2.0691\n","[Iter 63 Task norm] Train Loss: 0.1382\n","[Iter 63 Task dept] Train Loss: 1.2017\n","[Iter 63 Total] Train Loss: 3.4091\n","======================================================================\n","[Iter 63 Total] Train Loss: 3.4091\n","i = 63\n","starting train step\n","[3.8646931648254395]\n","[Iter 64 Task segm] Train Loss: 1.6050\n","[Iter 64 Task norm] Train Loss: 0.0746\n","[Iter 64 Task dept] Train Loss: 2.1851\n","[Iter 64 Total] Train Loss: 3.8647\n","======================================================================\n","[Iter 64 Total] Train Loss: 3.8647\n","i = 64\n","starting train step\n","[3.324303150177002]\n","[Iter 65 Task segm] Train Loss: 2.5206\n","[Iter 65 Task norm] Train Loss: 0.0525\n","[Iter 65 Task dept] Train Loss: 0.7512\n","[Iter 65 Total] Train Loss: 3.3243\n","======================================================================\n","[Iter 65 Total] Train Loss: 3.3243\n","i = 65\n","starting train step\n","[5.181264400482178]\n","[Iter 66 Task segm] Train Loss: 3.1153\n","[Iter 66 Task norm] Train Loss: 0.0496\n","[Iter 66 Task dept] Train Loss: 2.0164\n","[Iter 66 Total] Train Loss: 5.1813\n","======================================================================\n","[Iter 66 Total] Train Loss: 5.1813\n","i = 66\n","starting train step\n","[3.833742141723633]\n","[Iter 67 Task segm] Train Loss: 1.9992\n","[Iter 67 Task norm] Train Loss: 0.0971\n","[Iter 67 Task dept] Train Loss: 1.7374\n","[Iter 67 Total] Train Loss: 3.8337\n","======================================================================\n","[Iter 67 Total] Train Loss: 3.8337\n","i = 67\n","starting train step\n","[3.927664279937744]\n","[Iter 68 Task segm] Train Loss: 2.4397\n","[Iter 68 Task norm] Train Loss: 0.1530\n","[Iter 68 Task dept] Train Loss: 1.3349\n","[Iter 68 Total] Train Loss: 3.9277\n","======================================================================\n","[Iter 68 Total] Train Loss: 3.9277\n","i = 68\n","starting train step\n","[3.5704050064086914]\n","[Iter 69 Task segm] Train Loss: 1.9941\n","[Iter 69 Task norm] Train Loss: 0.0514\n","[Iter 69 Task dept] Train Loss: 1.5249\n","[Iter 69 Total] Train Loss: 3.5704\n","======================================================================\n","[Iter 69 Total] Train Loss: 3.5704\n","i = 69\n","starting train step\n","[4.727067470550537]\n","[Iter 70 Task segm] Train Loss: 3.2002\n","[Iter 70 Task norm] Train Loss: 0.1220\n","[Iter 70 Task dept] Train Loss: 1.4049\n","[Iter 70 Total] Train Loss: 4.7271\n","======================================================================\n","[Iter 70 Total] Train Loss: 4.7271\n","i = 70\n","starting train step\n","[5.471530914306641]\n","[Iter 71 Task segm] Train Loss: 3.7940\n","[Iter 71 Task norm] Train Loss: 0.1155\n","[Iter 71 Task dept] Train Loss: 1.5620\n","[Iter 71 Total] Train Loss: 5.4715\n","======================================================================\n","[Iter 71 Total] Train Loss: 5.4715\n","i = 71\n","starting train step\n","[4.43331241607666]\n","[Iter 72 Task segm] Train Loss: 2.7785\n","[Iter 72 Task norm] Train Loss: 0.0434\n","[Iter 72 Task dept] Train Loss: 1.6114\n","[Iter 72 Total] Train Loss: 4.4333\n","======================================================================\n","[Iter 72 Total] Train Loss: 4.4333\n","i = 72\n","starting train step\n","[4.287564754486084]\n","[Iter 73 Task segm] Train Loss: 2.8123\n","[Iter 73 Task norm] Train Loss: 0.2037\n","[Iter 73 Task dept] Train Loss: 1.2716\n","[Iter 73 Total] Train Loss: 4.2876\n","======================================================================\n","[Iter 73 Total] Train Loss: 4.2876\n","i = 73\n","starting train step\n","[4.575244903564453]\n","[Iter 74 Task segm] Train Loss: 2.1379\n","[Iter 74 Task norm] Train Loss: 0.0732\n","[Iter 74 Task dept] Train Loss: 2.3641\n","[Iter 74 Total] Train Loss: 4.5752\n","======================================================================\n","[Iter 74 Total] Train Loss: 4.5752\n","i = 74\n","starting train step\n","[4.648752212524414]\n","[Iter 75 Task segm] Train Loss: 3.1619\n","[Iter 75 Task norm] Train Loss: 0.1217\n","[Iter 75 Task dept] Train Loss: 1.3652\n","[Iter 75 Total] Train Loss: 4.6488\n","======================================================================\n","[Iter 75 Total] Train Loss: 4.6488\n","i = 75\n","starting train step\n","[5.640801429748535]\n","[Iter 76 Task segm] Train Loss: 3.4382\n","[Iter 76 Task norm] Train Loss: 0.0547\n","[Iter 76 Task dept] Train Loss: 2.1478\n","[Iter 76 Total] Train Loss: 5.6408\n","======================================================================\n","[Iter 76 Total] Train Loss: 5.6408\n","i = 76\n","starting train step\n","[3.4092018604278564]\n","[Iter 77 Task segm] Train Loss: 2.4557\n","[Iter 77 Task norm] Train Loss: 0.1006\n","[Iter 77 Task dept] Train Loss: 0.8529\n","[Iter 77 Total] Train Loss: 3.4092\n","======================================================================\n","[Iter 77 Total] Train Loss: 3.4092\n","i = 77\n","starting train step\n","[4.3297247886657715]\n","[Iter 78 Task segm] Train Loss: 3.1357\n","[Iter 78 Task norm] Train Loss: 0.0783\n","[Iter 78 Task dept] Train Loss: 1.1158\n","[Iter 78 Total] Train Loss: 4.3297\n","======================================================================\n","[Iter 78 Total] Train Loss: 4.3297\n","i = 78\n","starting train step\n","[4.020784854888916]\n","[Iter 79 Task segm] Train Loss: 2.6310\n","[Iter 79 Task norm] Train Loss: 0.0553\n","[Iter 79 Task dept] Train Loss: 1.3344\n","[Iter 79 Total] Train Loss: 4.0208\n","======================================================================\n","[Iter 79 Total] Train Loss: 4.0208\n","i = 79\n","starting train step\n","[3.2113797664642334]\n","[Iter 80 Task segm] Train Loss: 2.1630\n","[Iter 80 Task norm] Train Loss: 0.0459\n","[Iter 80 Task dept] Train Loss: 1.0025\n","[Iter 80 Total] Train Loss: 3.2114\n","======================================================================\n","[Iter 80 Total] Train Loss: 3.2114\n","i = 80\n","starting train step\n","[3.4442667961120605]\n","[Iter 81 Task segm] Train Loss: 2.2701\n","[Iter 81 Task norm] Train Loss: 0.0458\n","[Iter 81 Task dept] Train Loss: 1.1284\n","[Iter 81 Total] Train Loss: 3.4443\n","======================================================================\n","[Iter 81 Total] Train Loss: 3.4443\n","i = 81\n","starting train step\n","[2.913729190826416]\n","[Iter 82 Task segm] Train Loss: 1.8503\n","[Iter 82 Task norm] Train Loss: 0.1331\n","[Iter 82 Task dept] Train Loss: 0.9303\n","[Iter 82 Total] Train Loss: 2.9137\n","======================================================================\n","[Iter 82 Total] Train Loss: 2.9137\n","i = 82\n","starting train step\n","[3.8640599250793457]\n","[Iter 83 Task segm] Train Loss: 2.1534\n","[Iter 83 Task norm] Train Loss: 0.0949\n","[Iter 83 Task dept] Train Loss: 1.6158\n","[Iter 83 Total] Train Loss: 3.8641\n","======================================================================\n","[Iter 83 Total] Train Loss: 3.8641\n","i = 83\n","starting train step\n","[4.874311923980713]\n","[Iter 84 Task segm] Train Loss: 3.3991\n","[Iter 84 Task norm] Train Loss: 0.0434\n","[Iter 84 Task dept] Train Loss: 1.4318\n","[Iter 84 Total] Train Loss: 4.8743\n","======================================================================\n","[Iter 84 Total] Train Loss: 4.8743\n","i = 84\n","starting train step\n","[3.224109649658203]\n","[Iter 85 Task segm] Train Loss: 1.6163\n","[Iter 85 Task norm] Train Loss: 0.1011\n","[Iter 85 Task dept] Train Loss: 1.5067\n","[Iter 85 Total] Train Loss: 3.2241\n","======================================================================\n","[Iter 85 Total] Train Loss: 3.2241\n","i = 85\n","starting train step\n","[3.437380790710449]\n","[Iter 86 Task segm] Train Loss: 2.3852\n","[Iter 86 Task norm] Train Loss: 0.1400\n","[Iter 86 Task dept] Train Loss: 0.9122\n","[Iter 86 Total] Train Loss: 3.4374\n","======================================================================\n","[Iter 86 Total] Train Loss: 3.4374\n","i = 86\n","starting train step\n","[3.574389934539795]\n","[Iter 87 Task segm] Train Loss: 2.0609\n","[Iter 87 Task norm] Train Loss: 0.1003\n","[Iter 87 Task dept] Train Loss: 1.4132\n","[Iter 87 Total] Train Loss: 3.5744\n","======================================================================\n","[Iter 87 Total] Train Loss: 3.5744\n","i = 87\n","starting train step\n","[3.720348358154297]\n","[Iter 88 Task segm] Train Loss: 2.3018\n","[Iter 88 Task norm] Train Loss: 0.1050\n","[Iter 88 Task dept] Train Loss: 1.3135\n","[Iter 88 Total] Train Loss: 3.7203\n","======================================================================\n","[Iter 88 Total] Train Loss: 3.7203\n","i = 88\n","starting train step\n","[4.452972412109375]\n","[Iter 89 Task segm] Train Loss: 3.0984\n","[Iter 89 Task norm] Train Loss: 0.0729\n","[Iter 89 Task dept] Train Loss: 1.2816\n","[Iter 89 Total] Train Loss: 4.4530\n","======================================================================\n","[Iter 89 Total] Train Loss: 4.4530\n","i = 89\n","starting train step\n","[3.6106138229370117]\n","[Iter 90 Task segm] Train Loss: 2.2043\n","[Iter 90 Task norm] Train Loss: 0.0625\n","[Iter 90 Task dept] Train Loss: 1.3439\n","[Iter 90 Total] Train Loss: 3.6106\n","======================================================================\n","[Iter 90 Total] Train Loss: 3.6106\n","i = 90\n","starting train step\n","[3.6064157485961914]\n","[Iter 91 Task segm] Train Loss: 2.4122\n","[Iter 91 Task norm] Train Loss: 0.0729\n","[Iter 91 Task dept] Train Loss: 1.1213\n","[Iter 91 Total] Train Loss: 3.6064\n","======================================================================\n","[Iter 91 Total] Train Loss: 3.6064\n","i = 91\n","starting train step\n","[3.9258437156677246]\n","[Iter 92 Task segm] Train Loss: 2.4296\n","[Iter 92 Task norm] Train Loss: 0.0850\n","[Iter 92 Task dept] Train Loss: 1.4112\n","[Iter 92 Total] Train Loss: 3.9258\n","======================================================================\n","[Iter 92 Total] Train Loss: 3.9258\n","i = 92\n","starting train step\n","[5.191074848175049]\n","[Iter 93 Task segm] Train Loss: 3.2419\n","[Iter 93 Task norm] Train Loss: 0.0832\n","[Iter 93 Task dept] Train Loss: 1.8660\n","[Iter 93 Total] Train Loss: 5.1911\n","======================================================================\n","[Iter 93 Total] Train Loss: 5.1911\n","i = 93\n","starting train step\n","[3.8922104835510254]\n","[Iter 94 Task segm] Train Loss: 2.8537\n","[Iter 94 Task norm] Train Loss: 0.0285\n","[Iter 94 Task dept] Train Loss: 1.0100\n","[Iter 94 Total] Train Loss: 3.8922\n","======================================================================\n","[Iter 94 Total] Train Loss: 3.8922\n","i = 94\n","starting train step\n","[3.32539701461792]\n","[Iter 95 Task segm] Train Loss: 2.2403\n","[Iter 95 Task norm] Train Loss: 0.0230\n","[Iter 95 Task dept] Train Loss: 1.0620\n","[Iter 95 Total] Train Loss: 3.3254\n","======================================================================\n","[Iter 95 Total] Train Loss: 3.3254\n","i = 95\n","starting train step\n","[4.68722677230835]\n","[Iter 96 Task segm] Train Loss: 2.8927\n","[Iter 96 Task norm] Train Loss: 0.2489\n","[Iter 96 Task dept] Train Loss: 1.5456\n","[Iter 96 Total] Train Loss: 4.6872\n","======================================================================\n","[Iter 96 Total] Train Loss: 4.6872\n","i = 96\n","starting train step\n","[4.33977746963501]\n","[Iter 97 Task segm] Train Loss: 3.0446\n","[Iter 97 Task norm] Train Loss: 0.0390\n","[Iter 97 Task dept] Train Loss: 1.2561\n","[Iter 97 Total] Train Loss: 4.3398\n","======================================================================\n","[Iter 97 Total] Train Loss: 4.3398\n","i = 97\n","starting train step\n","[3.699251174926758]\n","[Iter 98 Task segm] Train Loss: 2.7967\n","[Iter 98 Task norm] Train Loss: 0.0921\n","[Iter 98 Task dept] Train Loss: 0.8104\n","[Iter 98 Total] Train Loss: 3.6993\n","======================================================================\n","[Iter 98 Total] Train Loss: 3.6993\n","i = 98\n","starting train step\n","[4.335464000701904]\n","[Iter 99 Task segm] Train Loss: 2.4548\n","[Iter 99 Task norm] Train Loss: 0.2096\n","[Iter 99 Task dept] Train Loss: 1.6710\n","[Iter 99 Total] Train Loss: 4.3355\n","======================================================================\n","[Iter 99 Total] Train Loss: 4.3355\n","i = 99\n","starting train step\n","[3.6044139862060547]\n","[Iter 100 Task segm] Train Loss: 2.3493\n","[Iter 100 Task norm] Train Loss: 0.1228\n","[Iter 100 Task dept] Train Loss: 1.1324\n","[Iter 100 Total] Train Loss: 3.6044\n","======================================================================\n","[Iter 100 Total] Train Loss: 3.6044\n"]}]},{"cell_type":"code","source":["%tensorboard --logdir runs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":839},"id":"9-FhhPYCFMxd","executionInfo":{"status":"ok","timestamp":1646009948264,"user_tz":300,"elapsed":142,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}},"outputId":"db8048cc-9d0f-4a0a-91a3-b5ebf1e79634"},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 1285), started 2:19:35 ago. (Use '!kill 1285' to kill it.)"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["\n","        (async () => {\n","            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n","            url.searchParams.set('tensorboardColab', 'true');\n","            const iframe = document.createElement('iframe');\n","            iframe.src = url;\n","            iframe.setAttribute('width', '100%');\n","            iframe.setAttribute('height', '800');\n","            iframe.setAttribute('frameborder', 0);\n","            document.body.appendChild(iframe);\n","        })();\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}]},{"cell_type":"code","source":["# !tensorboard --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G2dzXRe7Tcfe","executionInfo":{"status":"ok","timestamp":1645987850794,"user_tz":300,"elapsed":4804,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}},"outputId":"f1f786da-be18-45c3-f607-5e17f795480d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-02-27 18:50:49.943161: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2.8.0\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"rxTcf5gshcrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !tensorboard --logdir=runs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0iymET_OiWjT","executionInfo":{"status":"ok","timestamp":1645988223445,"user_tz":300,"elapsed":74353,"user":{"displayName":"Saurabh Bajaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08233394637176775621"}},"outputId":"8551e183-60c9-4aac-b38d-f717668de4a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-02-27 18:55:52.553699: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","\n","NOTE: Using experimental fast data loading logic. To disable, pass\n","    \"--load_fast=false\" and report issues on GitHub. More details:\n","    https://github.com/tensorflow/tensorboard/issues/4784\n","\n","Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n","TensorBoard 2.8.0 at http://localhost:6006/ (Press CTRL+C to quit)\n","^C\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"tH-Gupo7ijC_"},"execution_count":null,"outputs":[]}]}